import torch                                                         
from transformers import pipeline
import os
import sys


# 1. Set model path (with safety check)

# Expected path where your fine-tuned model is stored
finetuned_model_path = "./ko-en-finetuned-model/final_model"
# Base model to use if the fine-tuned model is not found
base_model_name = "Helsinki-NLP/opus-mt-ko-en"

# Check whether the fine-tuned model actually exists
if os.path.exists(finetuned_model_path):
    print(f"âœ… Fine-tuned model found! Path: {finetuned_model_path}")
    target_model = finetuned_model_path
else:
    print(f"âš ï¸Fine-tuned model not found. (Path: {finetuned_model_path})")
    print(f"ğŸ‘‰ Falling back to the base model: {base_model_name}")
    target_model = base_model_name

# 2. Initialize translator

print("â³Loading translation model...")
device = 0 if torch.cuda.is_available() else -1

try:
    translator = pipeline(
        "translation",
        model=target_model,
        tokenizer=target_model,
        device=device
    )
except Exception as e:
    print(f"\n âŒFatal error occurred: {e}")
    sys.exit()


# 3. Sentences to test translation

sentences = [
    "Cì–¸ì–´ë¥¼ ê³µë¶€í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¦ê²ìŠµë‹ˆë‹¤."
    "ì‹œí—˜ì´ ë¹¨ë¦¬ ëë‚˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤."
]


# 4. Run translation & print results

print("\n" + "="*50)
print(f"   [ Translation Test Results ] (Model used: {target_model})")
print("="*50)

for text in sentences:
    # Perform translation
    result = translator(text)
    translated_text = result[0]['translation_text']

    # Print result
    print(f"ğŸ‡°ğŸ‡· Korean Input : {text}")
    print(f"ğŸ‡ºğŸ‡¸ English Output : {translated_text}")
    print("-" * 50)

print("\n âœ…Test Completed!")



